{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ehM2vGe2uTYAxKOVkP9Y_O2KC-s3aA11",
      "authorship_tag": "ABX9TyPOFZIMAzsT+Vidi+lU2iLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Untick/InspectrumClinic_RecSys/blob/main/Zemlyakov_Serj/Int_1_%D0%9F%D1%80%D0%BE%D1%84%D0%BF%D0%B0%D1%82%D0%BE%D0%BB%D0%BE%D0%B3_2_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интеграция модели для предсказания"
      ],
      "metadata": {
        "id": "YRD5ldkeAGMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Работа с массивами данных\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Регулярные выражения\n",
        "import re\n",
        "\n",
        "#Для работы с файлами \n",
        "import os \n",
        "\n",
        "# Модуль для работы с zip-архивами\n",
        "from zipfile import ZipFile as Zip             \n",
        "\n",
        "# Работа с табличными данными\n",
        "import pandas as pd\n",
        "\n",
        "# Функции-утилиты для работы с категориальными данными\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "# Токенизатор для преобразования текстов в последовательности\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Масштабирование данных\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE3kt30bDuUN",
        "outputId": "e3a44cc0-7997-4e1e-cc73-5fb3ecc4ae97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhzRcar5Ky-O"
      },
      "source": [
        "# Задаем константы\n",
        "# Количество классов\n",
        "CLASS_COUNT = 3\n",
        "\n",
        "# Задаем словарь классов\n",
        "CLASS_DICT ={'Негоден':0, 'ОграниченноГоден':1, 'Годен':2}\n",
        "\n",
        "# Задаем словарь групп здоровья\n",
        "GZ_DICT ={'':0, 'I':1, 'II':2, 'IIIа':3, 'IIIб':4}\n",
        "\n",
        "# Функция извлечения данных о вредных факторах\n",
        "\n",
        "def extract_harm_text(df_harm):\n",
        "    result = []\n",
        "    COL_HARM   = df_harm.columns.get_loc('Вредность')\n",
        "    # Для всех строк таблицы: собрать значения столбцов вредности\n",
        "    for row in df_harm.values:\n",
        "                    result.append(str(row[COL_HARM]).split(','))\n",
        "    # Возврат в виде массива\n",
        "    return result\n",
        "\n",
        "# Функция извлечения данных о Диагнозе\n",
        "\n",
        "def extract_Diag_text(df_diag):\n",
        "    result = []\n",
        "    COL_DIAG    = df_diag.columns.get_loc('Диагнозы')\n",
        "    # Для всех строк таблицы: собрать значения сводного Диагноза\n",
        "    for row in df_diag.values:\n",
        "                    result.append(str(row[COL_DIAG]).split(','))\n",
        "    # Возврат в виде массива\n",
        "    return result\n",
        "\n",
        "# Функция извлечения данных о Рекомендациях\n",
        "\n",
        "def extract_recom_text(df_diag):\n",
        "    result = []\n",
        "    COL_RECOM    = df_diag.columns.get_loc('Рекомендации')\n",
        "    # Для всех строк таблицы: собрать значения сводного Диагноза\n",
        "    for row in df_diag.values:\n",
        "                    result.append(str(row[COL_RECOM]).split(','))\n",
        "    # Возврат в виде массива\n",
        "    return result\n",
        "\n",
        "# Функция извлечения данных о Группе здоровья OHE\n",
        "\n",
        "def extract_gz(df):\n",
        "  GZ_COUNT = 5\n",
        "  gz_list_key = list(df.ГруппаЗдоровья.values)\n",
        "  gz_list = []\n",
        "  for GZ in gz_list_key:\n",
        "    gz_list.append(GZ_DICT[GZ])\n",
        "  \n",
        "  gz_data = np.array(gz_list)            # Перевод общего списка меток класса в numpy-массив\n",
        "  gz_res = utils.to_categorical(gz_data, GZ_COUNT)\n",
        "  return gz_res\n",
        "\n",
        "# Функция перевода классов в OHE\n",
        "def Y_to_OHE(df):\n",
        "  y_list_key = list(df.ЗаключениеМК.values)\n",
        "  # print (y_list_key)\n",
        "  y_list = []\n",
        "  for CD in y_list_key:\n",
        "    y_list.append(CLASS_DICT[CD])\n",
        "  # print (y_list)\n",
        "\n",
        "  y_data = np.array(y_list)            # Перевод общего списка меток класса в numpy-массив\n",
        "\n",
        "  y_res = utils.to_categorical(y_data, CLASS_COUNT)\n",
        "  return y_data, y_res\n",
        "\n",
        "# Преобразование текстовых данных в числовые/векторные для обучения нейросетью\n",
        "# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря\n",
        "\n",
        "# Частотный словарь вредности\n",
        "tokenizer_harm = Tokenizer(num_words=200, # объем словаря\n",
        "                      filters='!\"#$%&()*+,-–—/:;<=>?@[\\\\]^_`{|}~\\t\\n\\xa0', # убираемые из текста ненужные символы \n",
        "                      lower=True, # приведение слов к нижнему регистру\n",
        "                      split=' ', # разделитель слов\n",
        "                      oov_token='unknown', # указание разделять по словам, а не по единичным символам\n",
        "                      char_level=False # токен для слов, которые не вошли в словарь\n",
        "                      )\n",
        "\n",
        "# Частотный словарь диагнозов узких специалистов\n",
        "tokenizer_Diag = Tokenizer(num_words=500, # объем словаря\n",
        "                      filters='!\"#$%&()*+,-–—./:;<=>?@[\\\\]^_`{|}~\\t\\n\\xa0', # убираемые из текста ненужные символы \n",
        "                      lower=False, # приведение слов к нижнему регистру\n",
        "                      split=' ', # разделитель слов\n",
        "                      oov_token='unknown', # указание разделять по словам, а не по единичным символам\n",
        "                      char_level=False # токен для слов, которые не вошли в словарь\n",
        "                      )\n",
        "\n",
        "# Частотный словарь рекомендаций узких специалистов\n",
        "tokenizer_recom = Tokenizer(num_words=100, # объем словаря\n",
        "                      filters='!\"#$%&()*+,-–—./:;<=>?@[\\\\]^_`{|}~\\t\\n\\xa0', # убираемые из текста ненужные символы \n",
        "                      lower=False, # приведение слов к нижнему регистру\n",
        "                      split=' ', # разделитель слов\n",
        "                      oov_token='unknown', # указание разделять по словам, а не по единичным символам\n",
        "                      char_level=False # токен для слов, которые не вошли в словарь\n",
        "                      )\n",
        "\n",
        "# Вспомогательные функции для очистки строковых данных для приведения к удобномы для обратки виду\n",
        "def purify(x):\n",
        "    if isinstance(x, str):                # Если значение - строка:\n",
        "        # Замена концов строк на пробелы, удаление символа с кодом 0xA0,\n",
        "        # обрезка краевых пробелов и приведение к нижнему регистру\n",
        "        x1 = re.sub(r',\\s*(?=,|$)', '', x).strip(',')\n",
        "    return x1\n",
        "\n",
        "def clean_string(text): \n",
        "    # удаление знаков препинания\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text    \n",
        "\n",
        "def clean_dict(dict_d):\n",
        "  advanceddict = {}\n",
        "  for key in dict_d:\n",
        "    # проверяем, есть ли в ключе разделитель \",\"\n",
        "    if re.search(',', key):\n",
        "      # если есть, то разделяем ключ на список из отдельных ключей\n",
        "      subkeys = key.split(',')\n",
        "      # для каждого нового ключа создаем запись в новом словаре\n",
        "      for subkey in subkeys:\n",
        "        # копируем значение из старого словаря\n",
        "        advanceddict[subkey] = dict_d[key]\n",
        "    else:\n",
        "      # если разделителя нет, то просто копируем ключ и значение\n",
        "      advanceddict[key] = dict_d[key]\n",
        "\n",
        "  return advanceddict\n",
        "\n",
        "   \n",
        "\n",
        "# Функция обработки столбца сводного диагноза\n",
        "def diag_adv (df0):\n",
        "  \n",
        "  df1 = df0.copy()\n",
        "  # Получим список всех сводных диагнозов \n",
        "  list_of_Diag = df1['Свод_Диагноз'].tolist()\n",
        "  # Используя заготовленную функцию, очистим каждую строку от \"мусора\"\n",
        "  clearlistdiag=[]\n",
        "  for A in list_of_Diag:\n",
        "    clearlistdiag.append(purify(A))\n",
        "  # Поместим в новую колонку очищенные строки из полученного списка\n",
        "  df1['Диагнозы'] = True\n",
        "  df1['Диагнозы'] = clearlistdiag\n",
        "\n",
        "  # Получаем новую таблицу данных\n",
        "  df1=df1[[\n",
        "      'Клиент',\n",
        "      'КлиентДатаРождения',\n",
        "      'ЗаключениеМК',\n",
        "      'ГруппаЗдоровья',\n",
        "      'Вредность',\n",
        "      'Диагнозы',\n",
        "      'Рекомендации']]\n",
        "  return df1     \n",
        "\n",
        "# Функция обработки столбца рекомендаций\n",
        "def recom_adv (df1):\n",
        "    df2 = df1.copy()\n",
        "    # Получим список всех рекомендаций \n",
        "    list_of_recom = df2['Рекомендации'].tolist()\n",
        "    # Используя заготовленную функцию, очистим каждую строку от \"мусора\"\n",
        "    clearlistrecom=[]\n",
        "    for A in list_of_recom:\n",
        "      clearlistrecom.append(purify(A))\n",
        "    # Поместим в новую колонку очищенные строки из полученного списка\n",
        "    df2['Рекомендации'] = True\n",
        "    df2['Рекомендации'] = clearlistrecom\n",
        "\n",
        "    # Получаем новую таблицу данных\n",
        "    df2=df2[[\n",
        "        'Клиент',\n",
        "        'КлиентДатаРождения',\n",
        "        'ЗаключениеМК',\n",
        "        'ГруппаЗдоровья',\n",
        "        'Вредность',\n",
        "        'Диагнозы',\n",
        "        'Рекомендации'\n",
        "        ]]\n",
        "    return df2       \n",
        "\n",
        "# Сформируем отдельную функцию, которая по передаваемой исходной таблицы создает сжатую\n",
        "def getclin(df_in):\n",
        "  # Получаем на вход некую таблицу данных вида аналогичного первоначально предоставленной\n",
        "  # Подготовка таблицы данных\n",
        "  # Убираем полностью пустые строки\n",
        "  df_in = df_in.dropna(axis=1, how='all')\n",
        "\n",
        "  # Убираем строки в которых отстутствует одно из встреченных заключений Профпатолога (ПП)\n",
        "  df_in = df_in[df_in['ЗаключениеМК'].isin(['Годен',\n",
        "                                      'ГоденСКоррекциейЗрения',\n",
        "                                      'ВременноНегоден',\n",
        "                                      'НуждаетсяВДообследованииИЛечении',\n",
        "                                      'ОграниченноГоден',\n",
        "                                      'ГоденБезРаботНаВысотах'])]\n",
        "\n",
        "  # Приводим к 3-м классам (Годен, Негоден, ОграниченноГоден)\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'ВременноНегоден', 'ЗаключениеМК'] = 'Негоден'\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'НуждаетсяВДообследованииИЛечении', 'ЗаключениеМК'] = 'Негоден'\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'ГоденСКоррекциейЗрения', 'ЗаключениеМК'] = 'ОграниченноГоден'\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'ГоденБезРаботНаВысотах', 'ЗаключениеМК'] = 'ОграниченноГоден'\n",
        "\n",
        "  # Для удобства обращения к столбцу, переименуем его в название из одного слова\n",
        "  df_in.rename(columns = {'Вредные Факторы':'Вредность'}, inplace = True )\n",
        "  df_in.rename(columns = {'Диспансерная Группа':'ГруппаЗдоровья'}, inplace = True )\n",
        "\n",
        "  # Оставляем только значащие для нас колонки: Заключение ПП, Вредность и Диагнозы узких специалистов в текстовом представлении\n",
        "  df_in = df_in[[\n",
        "  'Клиент',\n",
        "  'КлиентДатаРождения',\n",
        "\n",
        "  'ЗаключениеМК',\n",
        "\n",
        "  'ГруппаЗдоровья',\n",
        "  'Вредность',\n",
        "\n",
        "  'Психиатрия Наркология1 Диагноз Представление1',\n",
        "  'Офтальмология1_ДиагнозПредставление1',\n",
        "  'Оториноларингология1_ДиагнозПредставление1',\n",
        "  'Гинекология1_ДиагнозПредставление1',\n",
        "  'Стоматология1_ДиагнозПредставление1',\n",
        "  'Дерматовенерология1_ДиагнозПредставление1',\n",
        "  'Рентгенология1_ДиагнозПредставление1',\n",
        "  'Неврология1_ДиагнозПредставление1',\n",
        "  'Терапия1_ДиагнозПредставление1',\n",
        "  'Хирургия1_ДиагнозПредставление1',\n",
        "  'Офтальмология1_ДиагнозПредставление2',\n",
        "  'Терапия2_ДиагнозПредставление1',\n",
        "\n",
        "  'ЛабораторныеИсследования1_ХР_Рекомендация1Представление',\n",
        "  'Рентгенология1_Заключение_ЗначениеПредставление',\n",
        "  'ПсихиатрияНаркология1_ХР_Рекомендация1Представление',\n",
        "  'Офтальмология1_ХР_Рекомендация1Представление',\n",
        "  'Офтальмология1_ХР_Рекомендация2Представление',\n",
        "  'Оториноларингология1_ХР_Рекомендация1Представление',\n",
        "  'Гинекология1_ХР_Рекомендация1Представление',\n",
        "  'Гинекология1_ХР_Рекомендация2Представление',\n",
        "  'ФункциональнаяДиагностика1_ХР_Рекомендация1Представление',\n",
        "  'Стоматология2_ХР_Рекомендация1Представление',\n",
        "  'Терапия1_ХР_Рекомендация1Представление',\n",
        "  'Терапия1_ХР_Рекомендация2Представление',\n",
        "  'Дерматовенерология1_ХР_Рекомендация1Представление',\n",
        "  'Дерматовенерология1_ХР_Рекомендация2Представление',\n",
        "  'Неврология1_ХР_Рекомендация1Представление',\n",
        "  'Хирургия1_ХР_Рекомендация1Представление',\n",
        "  'Терапия2_ХР_Рекомендация1Представление',\n",
        "  'Терапия2_ХР_Рекомендация2Представление'\n",
        "  ]]\n",
        "\n",
        "  # Заменяем отсутсвтующие записи узких специалистов в пробелы\n",
        "  df_in=df_in.fillna('')\n",
        "\n",
        "  # Обобщенный диагноз - собирем диагнозы узких специалистов в одну строку через запятую\n",
        "  df_in['Свод_Диагноз'] = (df_in['Психиатрия Наркология1 Диагноз Представление1'].map(str) + ','\n",
        "  + df_in['Офтальмология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Оториноларингология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Гинекология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Стоматология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Дерматовенерология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Рентгенология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Неврология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Терапия1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Хирургия1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Офтальмология1_ДиагнозПредставление2'].map(str)+ ','\n",
        "  + df_in['Терапия2_ДиагнозПредставление1'].map(str))\n",
        "\n",
        "\n",
        "\n",
        "  # Обобщенная рекомендация\n",
        "  df_in['Рекомендации'] = (df_in['ЛабораторныеИсследования1_ХР_Рекомендация1Представление'].map(str) + ','\n",
        "  + df_in['Рентгенология1_Заключение_ЗначениеПредставление'].map(str) + ','\n",
        "  + df_in['ПсихиатрияНаркология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Офтальмология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Офтальмология1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['Оториноларингология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Гинекология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Гинекология1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['ФункциональнаяДиагностика1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Стоматология2_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['Дерматовенерология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Дерматовенерология1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['Неврология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Хирургия1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия2_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия2_ХР_Рекомендация2Представление'].map(str))\n",
        "\n",
        "\n",
        "  # Упрвостим таблицу данных до 7 столбцов\n",
        "  df_in1 = df_in[[\n",
        "      'Клиент',\n",
        "      'КлиентДатаРождения',\n",
        "      'ЗаключениеМК',\n",
        "      'ГруппаЗдоровья',\n",
        "      'Вредность',\n",
        "      'Свод_Диагноз',\n",
        "      'Рекомендации'\n",
        "      ]]\n",
        "\n",
        "  # Получаем новую таблицу данных\n",
        "  df_res1=diag_adv(df_in1)\n",
        "  df_res2=recom_adv(df_res1)\n",
        "\n",
        "  # Удаляем строки с отстуствующими записями всех узких специалистов                    \n",
        "  df_res = df_res2[df_res2['Диагнозы']!='']\n",
        "\n",
        "  # обновляем номера индексов\n",
        "  # df_res=df_res3.reset_index(drop=True)\n",
        "  return df_res\n",
        "\n",
        "# Функция преобразования данных из таблицы во входные данные для модели\n",
        "def getXdata (df_res):\n",
        "  # Извлечение текстов о вредности для выборки\n",
        "  harm_text = extract_harm_text(df_res) \n",
        "\n",
        "  tokenizer_harm.fit_on_texts(harm_text)\n",
        "  items_harm = list(tokenizer_harm.word_index.items())\n",
        "\n",
        "  # Преобразование текстов в последовательность индексов согласно частотному словарю\n",
        "  harm_seq = tokenizer_harm.texts_to_sequences(harm_text)\n",
        "\n",
        "  # Преобразование последовательностей индексов в bag of words\n",
        "  x_harm = tokenizer_harm.sequences_to_matrix(harm_seq)\n",
        "\n",
        "  # Извлечение текстов о сводном диагнозе для выборки\n",
        "  Diag_text = extract_Diag_text(df_res) \n",
        "\n",
        "  tokenizer_Diag.fit_on_texts(Diag_text)\n",
        "  items_Diag = list(tokenizer_Diag.word_index.items())\n",
        "\n",
        "  # Преобразование текстов в последовательность индексов согласно частотному словарю\n",
        "  Diag_seq = tokenizer_Diag.texts_to_sequences(Diag_text)\n",
        "\n",
        "  # Преобразование последовательностей индексов в bag of words\n",
        "  x_diag = tokenizer_Diag.sequences_to_matrix(Diag_seq)\n",
        "\n",
        "\n",
        "  # Извлечение текстов о рекомендациях для выборки\n",
        "  recom_text = extract_recom_text(df_res) \n",
        "\n",
        "  tokenizer_recom.fit_on_texts(recom_text)\n",
        "  items_recom = list(tokenizer_recom.word_index.items())\n",
        "\n",
        "  # Преобразование текстов в последовательность индексов согласно частотному словарю\n",
        "  recom_seq = tokenizer_recom.texts_to_sequences(recom_text)\n",
        "\n",
        "  # Преобразование последовательностей индексов в bag of words\n",
        "  x_recom = tokenizer_recom.sequences_to_matrix(recom_seq)\n",
        "\n",
        "  # Получение данных о группе здоровья\n",
        "  x_gz = extract_gz(df_res)\n",
        "   \n",
        "\n",
        "  return x_harm, x_diag, x_gz, x_recom\n",
        "\n",
        "\n",
        "def predict(df_path='/content/drive/MyDrive/internsheep1//profpat_1.csv', model_path='content/drive/MyDrive/internsheep1/model_work.h5'):\n",
        "    classes = {0: 'Негоден',\n",
        "               1: 'ОграниченноГоден',\n",
        "               2: 'Годен'}\n",
        "\n",
        "    # Загружаем тестовую таблицу данных от Заказчика(здесь для примера загружается та же таблица)\n",
        "    df_test = pd.read_csv('/content/drive/MyDrive/internsheep1//profpat_1.csv')\n",
        "\n",
        "    # Обрабатываем таблицу данных до необходимого формата\n",
        "    df_clin = getclin(df_test)\n",
        "\n",
        "    # Вредность, диагнозы, рекомендации от УС\n",
        "    x_test_harm, x_test_diag, x_test_gz, x_test_recom,  = getXdata (df_clin)\n",
        "\n",
        "    # Значения реальных классов для контроля точности в формате класса и векторного представления класса\n",
        "    y_data_test, y_vect_test = Y_to_OHE(df_clin)\n",
        "\n",
        "    # Формируем списки всех входных и выходных данных\n",
        "    y_vec_list = list(y_vect_test)\n",
        "    y_data_list = list(y_data_test)\n",
        "    client_list = list(df_clin['Клиент'])\n",
        "    clientDB_list = list(df_clin['КлиентДатаРождения'])\n",
        "    x_harm_list = list(df_clin['Вредность'])\n",
        "    x_diag_list = list(df_clin['Диагнозы'])\n",
        "    x_recom_list = list(df_clin['Рекомендации'])\n",
        "    x_gz_list = list(df_clin['ГруппаЗдоровья'])           \n",
        "    \n",
        "    # Загружаем модель\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    cls_df = np.argmax(model.predict([getXdata (df_clin)]), axis=-1)\n",
        "\n",
        "    # y_class_list =list(classes[cls_df])\n",
        "    y_class_list =list(cls_df)\n",
        "    # Сформируем таблицу с выводом реальных значений и предсказания\n",
        "    df_pred = pd.DataFrame({\n",
        "                      'РеалКласс': y_data_list,\n",
        "                      'Клиент': client_list,\n",
        "                      'КлиентДатаРождения': clientDB_list,\n",
        "                      'Заключение РС': y_class_list,\n",
        "                      'ГруппаЗдоровья': x_gz_list,\n",
        "                      'Вредный фактор': x_harm_list,\n",
        "                      'Диагнозы': x_diag_list,\n",
        "                      'Рекомендации': x_recom_list\n",
        "                      })\n",
        "    \n",
        "    # Вывод полученных данных в рекомендательную таблицу\n",
        "    df_pred.to_excel('pp_res.xlsx', index=False)    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JtnTMuV_Ng58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83838326-9ca2-467c-b715-95be5c0dfc3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-91ac4e17f1f8>:365: DtypeWarning: Columns (24,25,26,43,44,45,46,47,48,54,55,56,57,58,59,60,61,62,63,64,65,77,78,79,81,84,85,86,87,88,89,90,91,92,108,109,112,113,120,121,122,123,124,125,126,127,162,163,164,168,169,170,171,172,173,174,195,196,201,202,203,204,205,206,207,208,221,222,223,228,229,230,252,253,258,259,260,261,262,263,264,265,266,267,270,271,272,273,276,277,278,279,282,283,284,285,286,287,293,294,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,363,364,365,366,367,370,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,418,419,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,470,471,472,483,484,485,486,491,492,514,515,549,550,551,552,553,554,555,564,565,578,579,580,581,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,634,635,636,637,644,645,646,647,648,651,652,653,654,655,681,682,683,692,693,694,701,702,703,705,708,709,710,711,712,786,787,788,789,800,801,806,807,808,809,818,819,861,862,871,872,873,878,879,880,900,901,904,905,906,907,908,909,910,911,912,913,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,938,939,942,943,944,945,946,947,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1012,1013,1014,1021,1022,1023,1024,1028,1029,1030,1040,1041,1042,1043,1054,1055,1068,1069,1070,1071,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1139,1140,1141,1144,1145,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1237,1238,1239,1240,1243,1244,1245,1246,1259,1260,1261,1262,1263,1264,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1356,1357,1360,1361,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1470,1471,1472,1473,1474,1475,1481,1482,1492,1493,1494,1495,1496,1509,1510,1511,1512,1513,1514,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1538,1539,1594,1598,1599,1600,1601,1640,1641,1648,1649,1651,1652,1659,1660,1661,1662,1663,1664,1665,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1708,1709,1710,1712,1713,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1783,1784,1785,1786,1787,1788,1789,1790,1794,1795,1805,1806,1817,1818,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1890,1891,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1929,1930,1931,1934,1935,1936,1937,1938,1939,1964,1965,1966,1967,1968,1969,1970,1971,1977,1978,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2057,2058,2068,2069,2070,2071,2072,2082,2083,2084,2085,2086,2087,2088,2089,2090,2105,2106,2107,2108,2109,2110,2111,2112,2116,2117,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2254,2255,2256,2257,2258,2259,2269,2270,2271,2272,2273,2274,2275,2276,2277,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2305,2306,2307,2308,2309,2310,2311,2312,2322,2323,2324,2325,2326,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2370,2371,2373,2374,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2433,2434,2435,2436,2437,2440,2441,2446,2447,2448,2449,2450,2451,2452,2453,2454,2460,2461,2462,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2574,2575,2576,2577,2578,2579,2582,2583,2584,2585,2586,2587,2590,2591,2592,2593,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2609,2610,2611,2612,2613,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2672,2673,2674,2675,2676,2677,2686,2687,2688,2689,2690,2691,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2714,2715,2716,2717,2718,2719,2720,2721,2722,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2807,2808,2809,2810,2811,2812,2813,2815,2816,2823,2824,2825,2826,2827,2829,2830,2837,2838,2839,2840,2841,2851,2852,2853,2854,2855,2856,2865,2866,2867,2868,2869,2870,2879,2880,2881,2886,2887,2889,2890,2897,2898,2899,2900,2901,2911,2914,2915,2925,2928,2929,2939,2942,2943,2953,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2991,2992,2994,2995,3002,3003,3004,3005,3006,3008,3009,3016,3017,3018,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3114,3115,3117,3118,3125,3126,3127,3128,3129,3131,3132,3139,3140,3141,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3226,3227,3237,3238,3239,3242,3243,3244,3245,3255,3258,3259,3269,3272,3273,3283,3286,3287,3297,3300,3301,3302,3303,3304,3305,3315,3316,3317,3318,3319,3329,3330,3331,3332,3333,3341,3342,3343,3351,3352,3353,3354,3355,3356,3357,3358,3359,3369,3372,3373,3383,3386,3387,3397,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3457,3458,3459,3460,3461,3462,3471,3472,3473,3474,3475,3476,3485,3486,3487,3488,3489,3490,3499,3500,3501,3502,3503,3504,3505,3515,3516,3517,3518,3519,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3559,3560,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3610,3611,3621,3622,3623,3624,3625,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3719,3720,3730,3731,3732,3733,3734,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3836,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3973,3976,3977,3978,3979,3980,3989,3990,3991,3992,3993,4003,4004,4005,4008,4009,4019,4022,4023,4033,4034,4035,4036,4037,4038,4039,4040,4041,4047,4048,4058,4059,4060,4061,4062,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4128,4129,4130,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4158,4159,4160,4161,4162,4163,4164,4165,4166,4172,4173,4183,4184,4185,4186,4187,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4258,4259,4269,4270,4271,4272,4273,4283,4284,4285,4286,4287,4288,4289,4290,4291,4297,4298,4308,4309,4310,4311,4312,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4392,4393,4403,4404,4405,4406,4407,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4531,4532,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_test = pd.read_csv('/content/drive/MyDrive/internsheep1//profpat_1.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 [==============================] - 3s 44ms/step\n"
          ]
        }
      ],
      "source": [
        "predict(df_path='/content/drive/MyDrive/internsheep1/profpat_1.csv', model_path='/content/drive/MyDrive/internsheep1/model_18-90.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd1W8VDAN9ad"
      },
      "source": [
        "# Сохранение кода скрипта в переменной listing1\n",
        "\n",
        "listing1 = '''\n",
        "# Работа с массивами данных\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Регулярные выражения\n",
        "import re\n",
        "\n",
        "#Для работы с файлами \n",
        "import os \n",
        "\n",
        "# Модуль для работы с zip-архивами\n",
        "from zipfile import ZipFile as Zip             \n",
        "\n",
        "# Работа с табличными данными\n",
        "import pandas as pd\n",
        "\n",
        "# Функции-утилиты для работы с категориальными данными\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "# Токенизатор для преобразования текстов в последовательности\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Масштабирование данных\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from tensorflow_addons.metrics import F1Score\n",
        "\n",
        "# Задаем константы\n",
        "# Количество классов\n",
        "CLASS_COUNT = 3\n",
        "\n",
        "# Задаем словарь классов\n",
        "CLASS_DICT ={'Негоден':0, 'ОграниченноГоден':1, 'Годен':2}\n",
        "\n",
        "# Задаем словарь групп здоровья\n",
        "GZ_DICT ={'':0, 'I':1, 'II':2, 'IIIа':3, 'IIIб':4}\n",
        "\n",
        "# Функция извлечения данных о вредных факторах\n",
        "\n",
        "def extract_harm_text(df_harm):\n",
        "    result = []\n",
        "    COL_HARM   = df_harm.columns.get_loc('Вредность')\n",
        "    # Для всех строк таблицы: собрать значения столбцов вредности\n",
        "    for row in df_harm.values:\n",
        "                    result.append(str(row[COL_HARM]).split(','))\n",
        "    # Возврат в виде массива\n",
        "    return result\n",
        "\n",
        "# Функция извлечения данных о Диагнозе\n",
        "\n",
        "def extract_Diag_text(df_diag):\n",
        "    result = []\n",
        "    COL_DIAG    = df_diag.columns.get_loc('Диагнозы')\n",
        "    # Для всех строк таблицы: собрать значения сводного Диагноза\n",
        "    for row in df_diag.values:\n",
        "                    result.append(str(row[COL_DIAG]).split(','))\n",
        "    # Возврат в виде массива\n",
        "    return result\n",
        "\n",
        "# Функция извлечения данных о Рекомендациях\n",
        "\n",
        "def extract_recom_text(df_diag):\n",
        "    result = []\n",
        "    COL_RECOM    = df_diag.columns.get_loc('Рекомендации')\n",
        "    # Для всех строк таблицы: собрать значения сводного Диагноза\n",
        "    for row in df_diag.values:\n",
        "                    result.append(str(row[COL_RECOM]).split(','))\n",
        "    # Возврат в виде массива\n",
        "    return result\n",
        "\n",
        "# Функция извлечения данных о Группе здоровья OHE\n",
        "\n",
        "def extract_gz(df):\n",
        "  GZ_COUNT = 5\n",
        "  gz_list_key = list(df.ГруппаЗдоровья.values)\n",
        "  gz_list = []\n",
        "  for GZ in gz_list_key:\n",
        "    gz_list.append(GZ_DICT[GZ])\n",
        "  \n",
        "  gz_data = np.array(gz_list)            # Перевод общего списка меток класса в numpy-массив\n",
        "  gz_res = utils.to_categorical(gz_data, GZ_COUNT)\n",
        "  return gz_res\n",
        "\n",
        "# Функция перевода классов в OHE\n",
        "def Y_to_OHE(df):\n",
        "  y_list_key = list(df.ЗаключениеМК.values)\n",
        "  # print (y_list_key)\n",
        "  y_list = []\n",
        "  for CD in y_list_key:\n",
        "    y_list.append(CLASS_DICT[CD])\n",
        "  # print (y_list)\n",
        "\n",
        "  y_data = np.array(y_list)            # Перевод общего списка меток класса в numpy-массив\n",
        "\n",
        "  y_res = utils.to_categorical(y_data, CLASS_COUNT)\n",
        "  return y_data, y_res\n",
        "\n",
        "# Преобразование текстовых данных в числовые/векторные для обучения нейросетью\n",
        "# Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря\n",
        "\n",
        "# Частотный словарь вредности\n",
        "tokenizer_harm = Tokenizer(num_words=200, # объем словаря\n",
        "                      filters='!\"#$%&()*+,-–—/:;<=>?@[\\\\]^_`{|}~\\t\\n\\xa0', # убираемые из текста ненужные символы \n",
        "                      lower=True, # приведение слов к нижнему регистру\n",
        "                      split=' ', # разделитель слов\n",
        "                      oov_token='unknown', # указание разделять по словам, а не по единичным символам\n",
        "                      char_level=False # токен для слов, которые не вошли в словарь\n",
        "                      )\n",
        "\n",
        "# Частотный словарь диагнозов узких специалистов\n",
        "tokenizer_Diag = Tokenizer(num_words=500, # объем словаря\n",
        "                      filters='!\"#$%&()*+,-–—./:;<=>?@[\\\\]^_`{|}~\\t\\n\\xa0', # убираемые из текста ненужные символы \n",
        "                      lower=False, # приведение слов к нижнему регистру\n",
        "                      split=' ', # разделитель слов\n",
        "                      oov_token='unknown', # указание разделять по словам, а не по единичным символам\n",
        "                      char_level=False # токен для слов, которые не вошли в словарь\n",
        "                      )\n",
        "\n",
        "# Частотный словарь рекомендаций узких специалистов\n",
        "tokenizer_recom = Tokenizer(num_words=100, # объем словаря\n",
        "                      filters='!\"#$%&()*+,-–—./:;<=>?@[\\\\]^_`{|}~\\t\\n\\xa0', # убираемые из текста ненужные символы \n",
        "                      lower=False, # приведение слов к нижнему регистру\n",
        "                      split=' ', # разделитель слов\n",
        "                      oov_token='unknown', # указание разделять по словам, а не по единичным символам\n",
        "                      char_level=False # токен для слов, которые не вошли в словарь\n",
        "                      )\n",
        "\n",
        "# Вспомогательные функции для очистки строковых данных для приведения к удобномы для обратки виду\n",
        "def purify(x):\n",
        "    if isinstance(x, str):                # Если значение - строка:\n",
        "        # Замена концов строк на пробелы, удаление символа с кодом 0xA0,\n",
        "        # обрезка краевых пробелов и приведение к нижнему регистру\n",
        "        x1 = re.sub(r',\\s*(?=,|$)', '', x).strip(',')\n",
        "    return x1\n",
        "\n",
        "def clean_string(text): \n",
        "    # удаление знаков препинания\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text    \n",
        "\n",
        "def clean_dict(dict_d):\n",
        "  advanceddict = {}\n",
        "  for key in dict_d:\n",
        "    # проверяем, есть ли в ключе разделитель \",\"\n",
        "    if re.search(',', key):\n",
        "      # если есть, то разделяем ключ на список из отдельных ключей\n",
        "      subkeys = key.split(',')\n",
        "      # для каждого нового ключа создаем запись в новом словаре\n",
        "      for subkey in subkeys:\n",
        "        # копируем значение из старого словаря\n",
        "        advanceddict[subkey] = dict_d[key]\n",
        "    else:\n",
        "      # если разделителя нет, то просто копируем ключ и значение\n",
        "      advanceddict[key] = dict_d[key]\n",
        "\n",
        "  return advanceddict\n",
        "\n",
        "   \n",
        "\n",
        "# Функция обработки столбца сводного диагноза\n",
        "def diag_adv (df0):\n",
        "  \n",
        "  df1 = df0.copy()\n",
        "  # Получим список всех сводных диагнозов \n",
        "  list_of_Diag = df1['Свод_Диагноз'].tolist()\n",
        "  # Используя заготовленную функцию, очистим каждую строку от \"мусора\"\n",
        "  clearlistdiag=[]\n",
        "  for A in list_of_Diag:\n",
        "    clearlistdiag.append(purify(A))\n",
        "  # Поместим в новую колонку очищенные строки из полученного списка\n",
        "  df1['Диагнозы'] = True\n",
        "  df1['Диагнозы'] = clearlistdiag\n",
        "\n",
        "  # Получаем новую таблицу данных\n",
        "  df1=df1[[\n",
        "      'Клиент',\n",
        "      'КлиентДатаРождения',\n",
        "      'ЗаключениеМК',\n",
        "      'ГруппаЗдоровья',\n",
        "      'Вредность',\n",
        "      'Диагнозы',\n",
        "      'Рекомендации']]\n",
        "  return df1     \n",
        "\n",
        "# Функция обработки столбца рекомендаций\n",
        "def recom_adv (df1):\n",
        "    df2 = df1.copy()\n",
        "    # Получим список всех рекомендаций \n",
        "    list_of_recom = df2['Рекомендации'].tolist()\n",
        "    # Используя заготовленную функцию, очистим каждую строку от \"мусора\"\n",
        "    clearlistrecom=[]\n",
        "    for A in list_of_recom:\n",
        "      clearlistrecom.append(purify(A))\n",
        "    # Поместим в новую колонку очищенные строки из полученного списка\n",
        "    df2['Рекомендации'] = True\n",
        "    df2['Рекомендации'] = clearlistrecom\n",
        "\n",
        "    # Получаем новую таблицу данных\n",
        "    df2=df2[[\n",
        "        'Клиент',\n",
        "        'КлиентДатаРождения',\n",
        "        'ЗаключениеМК',\n",
        "        'ГруппаЗдоровья',\n",
        "        'Вредность',\n",
        "        'Диагнозы',\n",
        "        'Рекомендации'\n",
        "        ]]\n",
        "    return df2       \n",
        "\n",
        "# Сформируем отдельную функцию, которая по передаваемой исходной таблицы создает сжатую\n",
        "def getclin(df_in):\n",
        "  # Получаем на вход некую таблицу данных вида аналогичного первоначально предоставленной\n",
        "  # Подготовка таблицы данных\n",
        "  # Убираем полностью пустые строки\n",
        "  df_in = df_in.dropna(axis=1, how='all')\n",
        "\n",
        "  # Убираем строки в которых отстутствует одно из встреченных заключений Профпатолога (ПП)\n",
        "  df_in = df_in[df_in['ЗаключениеМК'].isin(['Годен',\n",
        "                                      'ГоденСКоррекциейЗрения',\n",
        "                                      'ВременноНегоден',\n",
        "                                      'НуждаетсяВДообследованииИЛечении',\n",
        "                                      'ОграниченноГоден',\n",
        "                                      'ГоденБезРаботНаВысотах'])]\n",
        "\n",
        "  # Приводим к 3-м классам (Годен, Негоден, ОграниченноГоден)\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'ВременноНегоден', 'ЗаключениеМК'] = 'Негоден'\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'НуждаетсяВДообследованииИЛечении', 'ЗаключениеМК'] = 'Негоден'\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'ГоденСКоррекциейЗрения', 'ЗаключениеМК'] = 'ОграниченноГоден'\n",
        "  df_in.loc[df_in['ЗаключениеМК'] == 'ГоденБезРаботНаВысотах', 'ЗаключениеМК'] = 'ОграниченноГоден'\n",
        "\n",
        "  # Для удобства обращения к столбцу, переименуем его в название из одного слова\n",
        "  df_in.rename(columns = {'Вредные Факторы':'Вредность'}, inplace = True )\n",
        "  df_in.rename(columns = {'Диспансерная Группа':'ГруппаЗдоровья'}, inplace = True )\n",
        "\n",
        "  # Оставляем только значащие для нас колонки: Заключение ПП, Вредность и Диагнозы узких специалистов в текстовом представлении\n",
        "  df_in = df_in[[\n",
        "  'Клиент',\n",
        "  'КлиентДатаРождения',\n",
        "\n",
        "  'ЗаключениеМК',\n",
        "\n",
        "  'ГруппаЗдоровья',\n",
        "  'Вредность',\n",
        "\n",
        "  'Психиатрия Наркология1 Диагноз Представление1',\n",
        "  'Офтальмология1_ДиагнозПредставление1',\n",
        "  'Оториноларингология1_ДиагнозПредставление1',\n",
        "  'Гинекология1_ДиагнозПредставление1',\n",
        "  'Стоматология1_ДиагнозПредставление1',\n",
        "  'Дерматовенерология1_ДиагнозПредставление1',\n",
        "  'Рентгенология1_ДиагнозПредставление1',\n",
        "  'Неврология1_ДиагнозПредставление1',\n",
        "  'Терапия1_ДиагнозПредставление1',\n",
        "  'Хирургия1_ДиагнозПредставление1',\n",
        "  'Офтальмология1_ДиагнозПредставление2',\n",
        "  'Терапия2_ДиагнозПредставление1',\n",
        "\n",
        "  'ЛабораторныеИсследования1_ХР_Рекомендация1Представление',\n",
        "  'Рентгенология1_Заключение_ЗначениеПредставление',\n",
        "  'ПсихиатрияНаркология1_ХР_Рекомендация1Представление',\n",
        "  'Офтальмология1_ХР_Рекомендация1Представление',\n",
        "  'Офтальмология1_ХР_Рекомендация2Представление',\n",
        "  'Оториноларингология1_ХР_Рекомендация1Представление',\n",
        "  'Гинекология1_ХР_Рекомендация1Представление',\n",
        "  'Гинекология1_ХР_Рекомендация2Представление',\n",
        "  'ФункциональнаяДиагностика1_ХР_Рекомендация1Представление',\n",
        "  'Стоматология2_ХР_Рекомендация1Представление',\n",
        "  'Терапия1_ХР_Рекомендация1Представление',\n",
        "  'Терапия1_ХР_Рекомендация2Представление',\n",
        "  'Дерматовенерология1_ХР_Рекомендация1Представление',\n",
        "  'Дерматовенерология1_ХР_Рекомендация2Представление',\n",
        "  'Неврология1_ХР_Рекомендация1Представление',\n",
        "  'Хирургия1_ХР_Рекомендация1Представление',\n",
        "  'Терапия2_ХР_Рекомендация1Представление',\n",
        "  'Терапия2_ХР_Рекомендация2Представление'\n",
        "  ]]\n",
        "\n",
        "  # Заменяем отсутсвтующие записи узких специалистов в пробелы\n",
        "  df_in=df_in.fillna('')\n",
        "\n",
        "  # Обобщенный диагноз - собирем диагнозы узких специалистов в одну строку через запятую\n",
        "  df_in['Свод_Диагноз'] = (df_in['Психиатрия Наркология1 Диагноз Представление1'].map(str) + ','\n",
        "  + df_in['Офтальмология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Оториноларингология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Гинекология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Стоматология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Дерматовенерология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Рентгенология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Неврология1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Терапия1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Хирургия1_ДиагнозПредставление1'].map(str)+ ','\n",
        "  + df_in['Офтальмология1_ДиагнозПредставление2'].map(str)+ ','\n",
        "  + df_in['Терапия2_ДиагнозПредставление1'].map(str))\n",
        "\n",
        "\n",
        "\n",
        "  # Обобщенная рекомендация\n",
        "  df_in['Рекомендации'] = (df_in['ЛабораторныеИсследования1_ХР_Рекомендация1Представление'].map(str) + ','\n",
        "  + df_in['Рентгенология1_Заключение_ЗначениеПредставление'].map(str) + ','\n",
        "  + df_in['ПсихиатрияНаркология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Офтальмология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Офтальмология1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['Оториноларингология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Гинекология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Гинекология1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['ФункциональнаяДиагностика1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Стоматология2_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['Дерматовенерология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Дерматовенерология1_ХР_Рекомендация2Представление'].map(str)+ ','\n",
        "  + df_in['Неврология1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Хирургия1_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия2_ХР_Рекомендация1Представление'].map(str)+ ','\n",
        "  + df_in['Терапия2_ХР_Рекомендация2Представление'].map(str))\n",
        "\n",
        "\n",
        "  # Упрвостим таблицу данных до 7 столбцов\n",
        "  df_in1 = df_in[[\n",
        "      'Клиент',\n",
        "      'КлиентДатаРождения',\n",
        "      'ЗаключениеМК',\n",
        "      'ГруппаЗдоровья',\n",
        "      'Вредность',\n",
        "      'Свод_Диагноз',\n",
        "      'Рекомендации'\n",
        "      ]]\n",
        "\n",
        "  # Получаем новую таблицу данных\n",
        "  df_res1=diag_adv(df_in1)\n",
        "  df_res2=recom_adv(df_res1)\n",
        "\n",
        "  # Удаляем строки с отстуствующими записями всех узких специалистов                    \n",
        "  df_res = df_res2[df_res2['Диагнозы']!='']\n",
        "\n",
        "  # обновляем номера индексов\n",
        "  # df_res=df_res3.reset_index(drop=True)\n",
        "  return df_res\n",
        "\n",
        "# Функция преобразования данных из таблицы во входные данные для модели\n",
        "def getXdata (df_res):\n",
        "  # Извлечение текстов о вредности для выборки\n",
        "  harm_text = extract_harm_text(df_res) \n",
        "\n",
        "  tokenizer_harm.fit_on_texts(harm_text)\n",
        "  items_harm = list(tokenizer_harm.word_index.items())\n",
        "\n",
        "  # Преобразование текстов в последовательность индексов согласно частотному словарю\n",
        "  harm_seq = tokenizer_harm.texts_to_sequences(harm_text)\n",
        "\n",
        "  # Преобразование последовательностей индексов в bag of words\n",
        "  x_harm = tokenizer_harm.sequences_to_matrix(harm_seq)\n",
        "\n",
        "  # Извлечение текстов о сводном диагнозе для выборки\n",
        "  Diag_text = extract_Diag_text(df_res) \n",
        "\n",
        "  tokenizer_Diag.fit_on_texts(Diag_text)\n",
        "  items_Diag = list(tokenizer_Diag.word_index.items())\n",
        "\n",
        "  # Преобразование текстов в последовательность индексов согласно частотному словарю\n",
        "  Diag_seq = tokenizer_Diag.texts_to_sequences(Diag_text)\n",
        "\n",
        "  # Преобразование последовательностей индексов в bag of words\n",
        "  x_diag = tokenizer_Diag.sequences_to_matrix(Diag_seq)\n",
        "\n",
        "\n",
        "  # Извлечение текстов о рекомендациях для выборки\n",
        "  recom_text = extract_recom_text(df_res) \n",
        "\n",
        "  tokenizer_recom.fit_on_texts(recom_text)\n",
        "  items_recom = list(tokenizer_recom.word_index.items())\n",
        "\n",
        "  # Преобразование текстов в последовательность индексов согласно частотному словарю\n",
        "  recom_seq = tokenizer_recom.texts_to_sequences(recom_text)\n",
        "\n",
        "  # Преобразование последовательностей индексов в bag of words\n",
        "  x_recom = tokenizer_recom.sequences_to_matrix(recom_seq)\n",
        "\n",
        "  # Получение данных о группе здоровья\n",
        "  x_gz = extract_gz(df_res)\n",
        "   \n",
        "\n",
        "  return x_harm, x_diag, x_gz, x_recom\n",
        "\n",
        "\n",
        "def predict(df_path='/content/drive/MyDrive/internsheep1//profpat_1.csv', model_path='content/drive/MyDrive/internsheep1/model_work.h5'):\n",
        "    classes = {0: 'Негоден',\n",
        "               1: 'ОграниченноГоден',\n",
        "               2: 'Годен'}\n",
        "\n",
        "    # Загружаем тестовую таблицу данных от Заказчика(здесь для примера загружается та же таблица)\n",
        "    df_test = pd.read_csv('/content/drive/MyDrive/internsheep1//profpat_1.csv')\n",
        "\n",
        "    # Обрабатываем таблицу данных до необходимого формата\n",
        "    df_clin = getclin(df_test)\n",
        "\n",
        "    # Вредность, диагнозы, рекомендации от УС\n",
        "    x_test_harm, x_test_diag, x_test_gz, x_test_recom,  = getXdata (df_clin)\n",
        "\n",
        "    # Значения реальных классов для контроля точности в формате класса и векторного представления класса\n",
        "    y_data_test, y_vect_test = Y_to_OHE(df_clin)\n",
        "\n",
        "    # Формируем списки всех входных и выходных данных\n",
        "    y_vec_list = list(y_vect_test)\n",
        "    y_data_list = list(y_data_test)\n",
        "    client_list = list(df_clin['Клиент'])\n",
        "    clientDB_list = list(df_clin['КлиентДатаРождения'])\n",
        "    x_harm_list = list(df_clin['Вредность'])\n",
        "    x_diag_list = list(df_clin['Диагнозы'])\n",
        "    x_recom_list = list(df_clin['Рекомендации'])\n",
        "    x_gz_list = list(df_clin['ГруппаЗдоровья'])           \n",
        "    \n",
        "    # Загружаем модель\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    cls_df = np.argmax(model.predict([getXdata (df_clin)]), axis=-1)\n",
        "\n",
        "    # y_class_list =list(classes[cls_df])\n",
        "    y_class_list =list(cls_df)\n",
        "    # Сформируем таблицу с выводом реальных значений и предсказания\n",
        "    df_pred = pd.DataFrame({\n",
        "                      'РеалКласс': y_data_list,\n",
        "                      'Клиент': client_list,\n",
        "                      'КлиентДатаРождения': clientDB_list,\n",
        "                      'Заключение РС': y_class_list,\n",
        "                      'ГруппаЗдоровья': x_gz_list,\n",
        "                      'Вредный фактор': x_harm_list,\n",
        "                      'Диагнозы': x_diag_list,\n",
        "                      'Рекомендации': x_recom_list\n",
        "                      })\n",
        "    \n",
        "    # Вывод полученных данных в рекомендательную таблицу\n",
        "    df_pred.to_excel('pp_res.xlsx', index=False)    \n",
        "    \n",
        "'''\n",
        "\n",
        "# Запись содержимого переменной в файл скрипта\n",
        "\n",
        "with open('script.py', 'w') as f:  # Создание / открытие файла \n",
        "    f.write(listing1)             # Запись в файл значения переменной text_code"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Загрузка модели\n",
        "model_download = tf.keras.models.load_model('/content/drive/MyDrive/internsheep1/model_18-90.h5')"
      ],
      "metadata": {
        "id": "iU77W-yBuSlK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Библиотека для работы с файлами\n",
        "from shutil import copyfile\n",
        "\n",
        "copyfile(f'/content/drive/MyDrive/internsheep1/profpat_1.csv', f'/content/profpat_1.csv')\n",
        "copyfile(f'/content/drive/MyDrive/internsheep1/model_18-90.h5', f'/content/model_18-90.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "22EFyerktZeK",
        "outputId": "1fcbdd2e-b5ab-4f72-cf8a-ca200181bc8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/model_18-90.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_txt = '''\n",
        "\n",
        "Таблица на 1980 записей пациентов\n",
        "\n",
        "'''\n",
        "\n",
        "with open('label.txt', 'w') as f:  # Создание / открытие файла \n",
        "    f.write(label_txt.strip())     # Запись в файл значения переменной"
      ],
      "metadata": {
        "id": "_GqDSoPWt-s2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile as Zip             # Модуль для работы с zip-архивами\n",
        "\n",
        "# Список всех файлов\n",
        "integrate_files = ['model_18-90.h5', 'script.py', 'label.txt', 'profpat_1.csv']\n",
        "\n",
        "zipArchive = Zip('/content/archive.zip', 'w')  # Открытие файла, если такого не существует - будет создан новый\n",
        "\n",
        "for file in integrate_files:                   # Для всех всем нужных файлов:\n",
        "    if os.path.exists(file):                   # Если файл существует,\n",
        "        zipArchive.write(file)                 # то добавление его в архив\n",
        "    else:\n",
        "        print(f'Файл {file} отсутствует!')     # иначе вывод на экран названия отсутствующего файла\n",
        "\n",
        "zipArchive.close()                             # В финале архив следует закрыть, как и обычный файл"
      ],
      "metadata": {
        "id": "Tl4HsfYIuNCS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copyfile(f'/content/archive.zip', f'/content/drive/MyDrive/internsheep1/archive.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NYKXqN7uxDed",
        "outputId": "54454c8c-45de-40e8-e4d3-60781fe1378a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/internsheep1/archive.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}